---
title: "Homework 5"
subtitle: "Bivariate Statistics One-way ANOVA and Regression Analysis"
date: 2024-11-22
date-format: long
author: 
  - Andri Setiyawan
  - Benedikt Meyer
  - Yosep Dwi Kristanto
format: pdf
number-sections: true
editor: visual
execute: 
  echo: false
editor_options: 
  chunk_output_type: console
---

```{r}
#| message: false

library(tidyverse)
library(haven)
library(knitr)
library(kableExtra)
library(ggforce)
library(ggpubr)
library(rstatix)
library(ggstatsplot)
library(broom)
library(ggtext)
```

::: {.callout-note icon="false"}
## Problems

1)  ANOVA: Launch SPSS and open the data file Telemarketing.sav

    *Assume that in an attempt to maximize profits, a telemarketing company is conducting an experiment to determine which of four scripted sales pitches generates the best revenue. 1500 different telemarketing calls are randomly assigned to one of the four scripts, and the resulting revenue for each call is recorded.*

    Run an appropriate ANOVA test for this research design.

2)  Regression Analysis: Run a multiple regression analysis on the examrevison.sav dataset, pay particular attention to the 7 Regression diagnostics conditions. This data represents measures from students used to predict how they perform in an exam.
:::

# Telemarketing

## Data Exploration

```{r}
telemarketing <- read_sav("Telemarketing.sav")
telemarketing <- telemarketing |> 
  mutate(
    sales_pitch = as_factor(sales_pitch),
    industry = as_factor(industry),
    region = as_factor(region)
  )
```

@tbl-summary-telemarketing shows that average revenue and variability differ across the four sales pitches, with Script A generating the highest average revenue and Script D the lowest.

```{r}
#| label: tbl-summary-telemarketing
#| tbl-cap: "Summary statistics for `revenue` (n, mean, and standard deviation) across different `sales_pitch` in the telemarketing data"

telemarketing |> 
  drop_na() |> 
  group_by(sales_pitch) |> 
  summarise(
    n = n(),
    avg_revenue = mean(revenue),
    sd_avenue = sd(revenue)
  ) |> 
  kbl(
    col.names = c("sales_pitch", "n", "M", "SD")
  ) |> 
  kable_styling()
```

The distribution of `revenue` across the four `sales_pitch` (Script A, Script B, Script C, and Script D) is visually summarized using violin plots combined with boxplots, as shown in @fig-dist-telemarketing.

```{r}
#| label: fig-dist-telemarketing
#| fig-cap: "Distribution of `revenue` across `sales_pitch` (Script A, Script B, Script C, and Script D) as illustrated by violin and boxplots."
#| fig-asp: 0.5625

telemarketing |> 
  drop_na() |> 
  ggplot(aes(
    x = sales_pitch,
    y = revenue
  )) + 
  geom_point(
    aes(color = sales_pitch),
    show.legend = FALSE,
    position = position_jitter(width = .1),
    size = 2,
    alpha = .5
  ) + 
  geom_violin(
    alpha = .3,
    width = .6,
    show.legend = FALSE
  ) + 
  geom_boxplot(
    width = .4,
    alpha = 0
  ) + 
  stat_summary(
    fun = "mean", geom = "point",
    size = 5, col = "darkred"
  ) + 
  scale_color_brewer(palette = "Dark2") + 
  theme_minimal() + 
  labs(
    x = "Sales Pitch",
    y = "Revenue"
  )
```

## Assumption Checking

-   The outcome variable, revenue, is measured on a ratio scale.

-   The groups are mutually exclusive, with four distinct categories: Script A, Script B, Script C, and Script D.

-   The grouping variable consists of four levels: Script A, Script B, Script C, and Script D.

-   The QQ plots were used to assess the normality of `revenue` distributions for each `sales_pitch` (Script A, Script B, Script C, and Script D). See @fig-qq-plot-telemarketing.

    ```{r}
    #| label: fig-qq-plot-telemarketing
    #| fig-cap: "QQ plot of `revenue` across `sales_pitch`"
    #| fig-asp: 1

    telemarketing |> 
      drop_na() |> 
      ggqqplot(
        x = "revenue",
        color = "sales_pitch"
      ) + 
      facet_wrap(vars(sales_pitch)) + 
      scale_color_brewer(palette = "Dark2") + 
      scale_fill_brewer(palette = "Dark2") + 
      theme_minimal() + 
      theme(
        legend.position = "none"
      )
    ```

    From @fig-qq-plot-telemarketing, it appears that the `revenue` for each `sales_pitch` is likely drawn from a normally distributed population. This observation is supported by the Shapiro-Wilk test results presented in @tbl-normality-test-telemarketing.

    ```{r}
    #| label: tbl-normality-test-telemarketing
    #| tbl-cap: "Shapiro-Wilk test of normality for `revenue` across `sales_pitch`"

    telemarketing |> 
      group_by(sales_pitch) |> 
      shapiro_test(revenue) |> 
      kbl() |> 
      kable_styling()
    ```

    The p-value in @tbl-normality-test-telemarketing is greater than .05 suggests that the `revenue` in each `sales_pitch` follows a normal distribution.

-   @tbl-levene-test-telemarketing presents the results of Levene's test for homogeneity of variances of `revenue` across the different `sales_pitch` groups. Since the p-value is greater than .05, it suggests that the assumption of equal variances is met.

    ```{r}
    #| label: tbl-levene-test-telemarketing
    #| tbl-cap: "Results of Levene test for homogeneity of variance"

    telemarketing |> 
      levene_test(revenue ~ sales_pitch) |> 
      kbl() |> 
      kable_styling()
    ```

## Hypotheses

$H_0$: The average `revenue` is equal across all `sales_pitch` groups.

$H_1$: At least one pair of `sales_pitch` groups has a different average `revenue`.

## Calculating the $F$ statistic

The ANOVA results in @tbl-aov-result-telemarketing show an F-value of 42.505, testing the difference in average `revenue` across the `sales_pitch` groups.

```{r}
#| label: tbl-aov-result-telemarketing
#| tbl-cap: "ANOVA table testing the difference in average `revenue` across `sales_pitch` groups."

aov_telemarketing <- telemarketing |> 
  drop_na() |> 
  anova_test(revenue ~ sales_pitch)
aov_telemarketing |>  kbl() |> 
  kable_styling()
```

## Testing for the significance of $F$

@tbl-aov-result-telemarketing shows a p-value of `r aov_telemarketing$p |> signif(3)`, which is less than .05. A visualization of the p-value is presented in @fig-p-telemarketing.

```{r}
#| label: fig-p-telemarketing
#| fig-cap: "Theoretical $F$-distribution with degrees of freedom 3 and 1484, illustrating the tail area corresponding to the p-value"
#| fig-asp: 0.5625

F_telemarketing <- aov_telemarketing$F |> 
  as.numeric() |> 
  round(2)
ggplot(data.frame(x = c(0, 50)), aes(x)) +
  stat_function(
    fun = df,
    args = list(df1 = 3, df2 = 1484),
    linewidth = 1
  ) + 
  stat_function(
    fun = df,
    args = list(df1 = 3, df2 = 1484),
    linewidth = 2,
    color = "blue",
    xlim = c(F_telemarketing, 50)
  ) + 
  geom_vline(
    xintercept = F_telemarketing,
    linetype = "dashed"
  ) + 
  geom_textbox(
    x = 46,
    y = .125,
    label = "Tail area is too small to see",
    maxwidth = .2
  ) + 
  scale_x_continuous(
    breaks = c(
      F_telemarketing,
      0, 10, 20, 30, 50
    )
  ) + 
  labs(
    x = "F-value",
    y = "Density"
  ) +
  theme_minimal()
```

## Interpreting $F$

Assuming the null hypothesis is true, i.e., that the average `revenue` is equal across all `sales_pitch` groups, the sample yields an F-statistic of `r aov_telemarketing$F |> round(2)` and a p-value of `r aov_telemarketing$p |> signif(2)`, which is less than .05. As a result, we reject the null hypothesis, indicating that at least two groups in the `sales_pitch` have different average `revenue`.

## Effect Size

The generalized eta-squared (ges) from @tbl-aov-result-telemarketing of `r aov_telemarketing$ges |> round(3)` indicates that approximately `r aov_telemarketing$ges |> round(3)*100`% of the total variance in `revenue` can be attributed to differences across the `sales_pitch` groups. The value of `r aov_telemarketing$ges |> round(3)` suggests a medium effect size, indicating that group differences in `sales_pitch` explain a meaningful but not overwhelming portion of the variance in `revenue`.

## Post-hoc test

@tbl-pairwise-telemarketing displays the pairwise t-test result with Bonferroni-adjusted p-values. It indicates the statistical differences between the groups. These results suggest that there are significant differences in average of `revenue` between each pair of `sales_pitch` groups, which are highlighted in the pairwise comparisons.

```{r}
#| label: tbl-pairwise-telemarketing
#| tbl-cap: "Pairwise t-test results comparing revenue between sales_pitch groups, with Bonferroni-adjusted p-values"

# Get unique combinations of groups
sales_pitch_combn <- combn(sort(unique(telemarketing$sales_pitch)), 2, simplify = FALSE)

# Perform pairwise t-tests and extract t-values
pair_t_telemarketing <- do.call(rbind, lapply(sales_pitch_combn, function(groups) {
  group1 <- telemarketing$revenue[telemarketing$sales_pitch == groups[1]]
  group2 <- telemarketing$revenue[telemarketing$sales_pitch == groups[2]]
  t_test <- t.test(group1, group2) # Perform t-test
  
  # Create a row of results
  data.frame(
    Group_1 = groups[1],
    Group_2 = groups[2],
    t = t_test$statistic,
    df = t_test$parameter
  )
}))

# Getting adjusted p-value
pair_t_p_telemarketing <- pairwise.t.test(
  telemarketing$revenue,
  telemarketing$sales_pitch,
  p.adjust.method = "bonferroni"
)$p.value |> 
  as.data.frame() |> 
  rownames_to_column(var = "Group_2") |> 
  pivot_longer(
    cols = `Script A`:`Script C`,
    names_to = "Group_1",
    values_to = "p"
  ) |> 
  mutate(
    p = as.character(signif(p, 3))
  ) |> 
  drop_na() |> 
  select(Group_1, Group_2, p)

# Combining t and p
pair_telemarketing <- pair_t_telemarketing |> 
  left_join(
    pair_t_p_telemarketing,
    by = join_by(Group_1, Group_2)
  ) |> tibble()

# Making a table
pair_telemarketing |> 
  kbl(
    col.names = c(
      "Group 1", "Group 2",
      "t", "df", "p (Bonferroni-adj.)"
    )
  ) |> 
  kable_styling()
```

The results of the pairwise t-tests are visually represented in @fig-aov-post-hoc-telemarketing.

```{r}
#| label: fig-aov-post-hoc-telemarketing
#| fig-cap: "Violin plots and boxplots showing the distribution of revenue across the sales_pitch groups, with results of pairwise t-tests indicating differences between groups."
#| fig-asp: 1
telemarketing |> 
  ggbetweenstats(
    x = sales_pitch,
    y = revenue,
    type = "parametric",
    pairwise.display = "significant",
    p.adjust.method = "bonferroni",
    effsize.type = "eta",
    bf.message = FALSE,
    var.equal = TRUE,
    xlab = "Sales Pitch",
    ylab = "Revenue"
  )
```

## Reporting the Results

A one-way analysis of variance (ANOVA) was conducted to examine the effect of sales pitch on revenue. The results revealed a significant difference in average revenue across the four sales pitch groups, F(3, 1484) = 42.51, p \< .001, generalized eta-squared (ges) = .079, indicating that sales pitch had a medium effect on revenue.

Post-hoc comparisons were performed using pairwise t-tests with Bonferroni correction to identify specific group differences. The results indicated the following:

-   The average revenue for Script A (M = 2970.63, SD = 947.23) was significantly higher than for Script B (M = 2669.13, SD = 970.92), t(602.47) = 3.92, p \< .001; Script C (M = 2471.29, SD = 967.06), t(579.28) = 6.30, p \< .001; and Script D (M = 2215.65, SD = 943.00), t(555.56) = 10.87, p \< .001.

-   Similarly, Script B had significantly higher revenue than Script C, t(641.98) = 2.61, p = .0495, and Script D, t(728.91) = 6.92, p \< .001.

-   Lastly, Script C had significantly higher revenue than Script D, t(613.53) = 3.74, p = .00109.

These results suggest that Script A consistently led to the highest revenue, while Script D resulted in the lowest revenue among the four groups.

# Students' Performance

```{r}
exam <- read_sav("examrevision (1).sav")
```

## Data Exploration

@fig-corr-mat-exam presents the relationships between `score`, `hours`, `anxiety`, and `a_points` in the exam dataset using Pearson correlation coefficients. The strongest positive correlation is observed between `score` and `a_points` (r = 0.87), followed by the correlation between `score` and `hours` (r = 0.82). A weaker, negative correlation is found between `hours` and `anxiety` (r = -0.34), as well as between `score` and `anxiety` (r = -0.12).

```{r}
#| label: fig-corr-mat-exam
#| fig-cap: "Correlation matrix among all variables in exam data"
#| fig-asp: 0.75

ggcorrmat(
  data = exam,
  cor.vars = score:a_points,
  p.adjust.method = "none"
)
```

@tbl-models-summary-exam presents the results of regression analyses with `score` as the dependent variable and combinations of `hours`, `anxiety`, and `a_points` as independent variables.

```{r}
#| label: tbl-models-summary-exam
#| tbl-cap: "Summary table of regression analyses for all possible combinations of independent variables (`hours`, `anxiety`, and `a_points`) predicting `score` in the `exam` dataset"

# Dependent variable
dep_var <- "score"

# Independent variables
indep_vars <- c("hours", "anxiety", "a_points")

# Generate all possible combinations of independent variables (1, 2, or 3)
all_models <- map(
  1:length(indep_vars),
  ~ combn(indep_vars, .x, simplify = FALSE)
) |> 
  unlist(recursive = FALSE)

# Fit models and extract statistics
results_model <- map_dfr(all_models, function(vars) {
  # Build formula
  formula <- as.formula(paste(dep_var, "~", paste(vars, collapse = " + ")))
  
  # Fit model
  model <- lm(formula, data = exam)
  
  # Extract statistics
  glance_summary <- glance(model)
  
  # Return a summary row
  tibble(
    dependent_variable = dep_var,
    independent_variables = paste(vars, collapse = ", "),
    F_statistic = glance_summary$statistic,
    p_value = glance_summary$p.value,
    R_squared = glance_summary$r.squared,
    df = glance_summary$df,
    df_res = glance_summary$df.residual
  )
})

# Print the summary table
results_model |> 
  kbl() |> 
  kable_styling()
```

We choose `hours` and `a_points` as the independent variables for further analysis because they individually and together demonstrate strong predictive power for score. The model with these two variables accounts for 83% of the variance and has a high F-statistic, suggesting they are reliable predictors without the redundancy of including less impactful variables like `anxiety`.

## Hypotheses

$H_0$: All regression coefficients are equal to zero (except the intercept).

$H_1$: At least one of the regression coefficients is not equal to zero.

## Assumption Checking

**Correct specification of the model:** It is make sense to predict students' performance score (`score`) with how long they spent on revision (`hours`) and their A-level entry points (`a_points`).

**Linearity:** @fig-linearity-exam shows relationships between `score`, `hours` and `a_points`. From the figures, we can see that the relationship between `hours`, `a_points`, and `score` are linear.

```{r}
#| label: fig-linearity-exam
#| fig-cap: "Linearty verification"
#| fig-subcap:
#|   - "Relationship between `hours` and `score`"
#|   - "Relationship between `a_points` and `score`"
#| fig-asp: 1
#| layout-ncol: 2

exam |> 
  ggplot(aes(x = hours, y = score)) + 
  geom_point(
    size = 3
  ) + 
  stat_ellipse(
    level = .99
  ) + 
  theme_minimal()
exam |> 
  ggplot(aes(x = a_points, y = score)) + 
  geom_point(
    size = 3
  ) + 
  stat_ellipse(
    level = .99
  ) + 
  theme_minimal()
```

**Measurement and normality of dependence variable:** The dependence variable, i.e. `score`, is ratio. From the @fig-qq-plot-score-exam, we can assume that `score` sample is from normally distributed population.

```{r}
#| label: fig-qq-plot-score-exam
#| fig-cap: "Assessing normality for `score`"
ggqqplot(
  data = exam,
  x = "score"
) + 
  theme_minimal()
```

**Absence of multicollinearity:** Here is the correlation coefficient between `hours` and `a_points`.

```{r}
cor(
  exam$hours,
  exam$a_points
)
```

Since the correlation coefficient is less than .8, we infer that there is no multicollinearity between the independent variables.

**Normal distribution of residuals:** @fig-qq-plot-resid-exam shows that the residuals are normally distributed.

```{r}
#| label: fig-qq-plot-resid-exam
#| fig-cap: "Assessing the normality of residuals"
#| fig-asp: 0.5625

reg_model_exam <- lm(
  score ~ hours + a_points,
  data = exam
)
data_model_exam <- tibble(
  score = exam$score,
  fitted = reg_model_exam$fitted.values |> as.numeric(),
  residuals = reg_model_exam$residuals |> as.numeric()
)
ggqqplot(
  data = data_model_exam,
  x = "residuals"
) + 
  theme_minimal()
```

**Homoscedasticity:** @fig-homo-resid-exam shows that the residuals have equal variance across dependence variable.

```{r}
#| label: fig-homo-resid-exam
#| fig-cap: "Assessing homoscedacity of residuals"
#| fig-asp: 0.5625

data_model_exam |> 
  ggplot(aes(x = fitted, y = residuals)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  theme_minimal()
```

## Model

Below is the result of a multiple regression analysis examining the relationship between `hours` and `a_points` as predictors of `score`. The regression model was statistically significant, with an F-statistic of 42.09 and a p-value of 2.604e-07, indicating that the model as a whole explains a significant portion of the variation in the dependent variable, `score`.

```{r}
summary(reg_model_exam)
```

The coefficient for `hours` was .4765 (t = 2.703, p = .0151), indicating that for each additional hour, the `score` is expected to increase by .4765, holding `a_points` constant. The coefficient for `a_points` was 1.9945 (t = 3.997, p = .000933), suggesting that for each additional point in `a_points`, the `score` is expected to increase by 1.9945, holding `hours` constant. The residual standard error was 4.751, and the model explained 83.2% of the variance in score (R-squared = 0.832).

@fig-3d-model-exam presents the multiple regression model with `hours` and `a_points` as independent variables and `score` as the dependent variable. The surface represents the predicted score across different combinations of `hours` and `a_points`. The red line segments in the plot depict the residuals, which represent the vertical distance between the observed data points and the corresponding predicted values on the surface.

::: {#fig-3d-model-exam layout-ncol="2"}
![](3d_model_1.png){#fig-3d-model-exam-1}

![](3d_model_2.png){#fig-3d-model-exam-2}

3D visualization of the multiple regression model examining the relationship between `hours` and `a_points` as predictors of `score`
:::
